{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------+--------------------+---------------+\n",
      "|                 _id|                date|    flag|       ids|                text|           user|\n",
      "+--------------------+--------------------+--------+----------+--------------------+---------------+\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:02:...|NO_QUERY|2059493951|Kinda scared to s...|          l7l7v|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:02:...|NO_QUERY|2059494141|RB TY@threebears:...|    DBSLKitties|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:02:...|NO_QUERY|2059494328|Trying to fix my ...|      AndrLMaxf|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:02:...|NO_QUERY|2059494463|@ether_radio no m...|   mumble_rosie|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:02:...|NO_QUERY|2059494807|i should be in lo...|         caavis|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:02:...|NO_QUERY|2059495145|Is having a weeke...|        lena247|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059496112|Going to watch Te...|     happytyper|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059496527|Why can't Sprinkl...|    jeanmatique|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059496589|@OGmuscles ugh I ...|     itskendall|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059497092|@NapaRegister I c...|      KelsOtter|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059497169|@stephenjeean he ...|      JennytG13|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059497205|Thought running w...|         KingSi|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059497280|wants to play gui...| joelszymanski7|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059497391|@Darshea15 Am I m...|    slimgoody05|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059497556|I knew I was goin...|  howmanyhearts|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059497818|    I got sunburned |xambermassie18x|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059498002|The park is gorge...|      jroforsho|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059498189|@palm 1 of the 1s...|       TgGlazer|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059498261|Watching Wildlife...|LeanneDeschanel|\n",
      "|{65891bb73fbab16a...|Sat Jun 06 17:03:...|NO_QUERY|2059498283|@grandsierra - I ...|    kathykaiser|\n",
      "+--------------------+--------------------+--------+----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import os\n",
    "import sys  # Add this import\n",
    "\n",
    "spark_home, java_home = \"C:/Spark\", \"C:/Java\" # Manually set Java and Spark home directory\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-memory 4g --executor-memory 4g pyspark-shell'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "os.environ['PATH'] = os.path.join(spark_home, 'bin') + os.pathsep + os.environ['PATH'] # Add the bin directory to the PATH\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('practice') \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\").getOrCreate()\n",
    "\n",
    "# Specify MongoDB connection options\n",
    "mongo_options = {\n",
    "    \"spark.mongodb.input.uri\": \"mongodb://localhost:27017/Big_Tweet.Tweets_Data\",}\n",
    "\n",
    "# Read data from MongoDB into a DataFrame\n",
    "df = spark.read.format(\"mongo\").options(**mongo_options).load()\n",
    "df.show() # Show the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to display the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    flag|\n",
      "+--------+\n",
      "|NO_QUERY|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to display the the distinct values\n",
    "distinct_flag = df.select(\"flag\").distinct()\n",
    "distinct_flag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n",
      "|                date|        user|                text|\n",
      "+--------------------+------------+--------------------+\n",
      "|Sat Jun 06 17:02:...|       l7l7v|Kinda scared to s...|\n",
      "|Sat Jun 06 17:02:...| DBSLKitties|RB TY@threebears:...|\n",
      "|Sat Jun 06 17:02:...|   AndrLMaxf|Trying to fix my ...|\n",
      "|Sat Jun 06 17:02:...|mumble_rosie|@ether_radio no m...|\n",
      "|Sat Jun 06 17:02:...|      caavis|i should be in lo...|\n",
      "+--------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to select the specific columns\n",
    "selected_data = df.select(\"date\", \"user\", \"text\")\n",
    "selected_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1600000\n"
     ]
    }
   ],
   "source": [
    "# to count the number of rows in the DataFrame\n",
    "row_count = selected_data.count()\n",
    "print(\"Number of rows:\", row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with duplicate and null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicate :-->  1600000\n",
      "Number of rows before after duplicate :--> 1598127\n"
     ]
    }
   ],
   "source": [
    "# to count the number of rows in the DataFrame\n",
    "old_count = selected_data.count()\n",
    "\n",
    "# Drop the null values from the cleaned data\n",
    "new_data = selected_data.dropDuplicates()\n",
    "\n",
    "# to count the number of rows in the DataFrame after removing the duplicates\n",
    "new_count = new_data.count()\n",
    "\n",
    "print(\"Number of rows before removing duplicate :--> \", old_count)\n",
    "print(\"Number of rows before after duplicate :-->\", new_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in text: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in a specific column (e.g., \"columnName\")\n",
    "column_name = \"text\"\n",
    "null_count_in_column = new_data.filter(col(column_name).isNull()).count()\n",
    "\n",
    "# Show the count of null values in the specific column\n",
    "print(f\"Number of null values in {column_name}: {null_count_in_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|          user|tweet_count|\n",
      "+--------------+-----------+\n",
      "|       Daniiej|          3|\n",
      "|           kdc|          9|\n",
      "|  jeffreeefans|         39|\n",
      "|     ASU_ConEd|          1|\n",
      "|    janiek1981|          2|\n",
      "|     taylajade|         15|\n",
      "|   SuperDunner|         14|\n",
      "|pantherapardus|         18|\n",
      "|    Peedletuck|          2|\n",
      "|     elimorris|          1|\n",
      "|    Calamorama|          1|\n",
      "|     AlexB1001|          1|\n",
      "|    macuser612|          2|\n",
      "|      tippi_jo|          2|\n",
      "|  ifyuhseekamy|          1|\n",
      "|   xsweetlukex|          2|\n",
      "|      chukaman|          7|\n",
      "|   beesarahlee|          1|\n",
      "|  reneewandell|          1|\n",
      "|  KatjaPresnal|          7|\n",
      "+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "grouped_df = new_data.groupBy(\"user\").agg(F.count(\"text\").alias(\"tweet_count\"))\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dealing with date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+\n",
      "|                date|         user|                text|\n",
      "+--------------------+-------------+--------------------+\n",
      "|Sat Jun 06 17:11:...|zorythevirgin|fml sending alott...|\n",
      "|Sat Jun 06 17:14:...|       KLoeff|I found yet anoth...|\n",
      "|Fri May 22 06:43:...|   MisterJLee|Morning sweethear...|\n",
      "|Fri May 22 06:43:...|      yelshia|Going camping! By...|\n",
      "|Sat May 30 09:48:...|      franzne|my finger is blee...|\n",
      "+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DateConversion\").getOrCreate()\n",
    "\n",
    "# Set the legacy time parser policy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Convert the string to a Unix timestamp\n",
    "selected_data = new_data.withColumn(\"unix_timestamp\", unix_timestamp(\"date\", \"EEE MMM dd HH:mm:ss zzz yyyy\"))\n",
    "\n",
    "# Convert Unix timestamp to the correct timestamp format\n",
    "selected_data = selected_data.withColumn(\"formatted_date\", from_unixtime(\"unix_timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Drop the original \"date\" column\n",
    "selected_data = selected_data.drop(\"date\")\n",
    "\n",
    "# Rename the \"formatted_date\" column to \"date\"\n",
    "selected_data = selected_data.withColumnRenamed(\"formatted_date\", \"date\")\n",
    "\n",
    "# Reorder the columns to have \"date\" at the first index\n",
    "selected_data = selected_data.select(\"date\", *selected_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------------------+\n",
      "|               date|         user|                text|\n",
      "+-------------------+-------------+--------------------+\n",
      "|2009-06-07 05:41:00|zorythevirgin|fml sending alott...|\n",
      "|2009-06-07 05:44:18|       KLoeff|I found yet anoth...|\n",
      "|2009-05-22 19:13:20|   MisterJLee|Morning sweethear...|\n",
      "|2009-05-22 19:13:20|      yelshia|Going camping! By...|\n",
      "|2009-05-30 22:18:16|      franzne|my finger is blee...|\n",
      "+-------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the first three columns and create a new DataFrame\n",
    "cleaned_data = selected_data.select(\"date\", \"user\", \"text\")\n",
    "cleaned_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+\n",
      "|               date|           user|                text|\n",
      "+-------------------+---------------+--------------------+\n",
      "|2009-04-07 10:49:45|_TheSpecialOne_|@switchfoot http:...|\n",
      "|2009-04-07 10:49:49|  scotthamilton|is upset that he ...|\n",
      "|2009-04-07 10:49:53|       mattycus|@Kenichan I dived...|\n",
      "|2009-04-07 10:49:57|         Karoli|@nationwideclass ...|\n",
      "|2009-04-07 10:49:57|        ElleCTF|my whole body fee...|\n",
      "+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame based on the \"date\" column\n",
    "sorted_data = cleaned_data.orderBy(\"date\")\n",
    "sorted_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import col, udf, regexp_replace, lower\n",
    "from pyspark.sql.types import StringType\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+--------------------+\n",
      "|               date|           user|                text|        cleaned_text|\n",
      "+-------------------+---------------+--------------------+--------------------+\n",
      "|2009-04-07 10:49:45|_TheSpecialOne_|@switchfoot http:...|                    |\n",
      "|2009-04-07 10:49:49|  scotthamilton|is upset that he ...|is upset that he ...|\n",
      "|2009-04-07 10:49:53|       mattycus|@Kenichan I dived...| i dived many tim...|\n",
      "|2009-04-07 10:49:57|         Karoli|@nationwideclass ...| no  not behaving...|\n",
      "|2009-04-07 10:49:57|        ElleCTF|my whole body fee...|my whole body fee...|\n",
      "|2009-04-07 10:50:00|       joy_wolf|@Kwesidei not the...| not the whole crew |\n",
      "|2009-04-07 10:50:03|           coZZ|@LOLTrish hey  lo...| hey  long time n...|\n",
      "|2009-04-07 10:50:03|        mybirch|         Need a hug |         need a hug |\n",
      "|2009-04-07 10:50:05|2Hood4Hollywood|@Tatiana_K nope t...| nope they  have it |\n",
      "|2009-04-07 10:50:09|        mimismo|@twittera que me ...|      que me muera  |\n",
      "|2009-04-07 10:50:16| erinx3leannexo|spring break in p...|spring break in p...|\n",
      "|2009-04-07 10:50:17|   pardonlauren|I just re-pierced...|i just re-pierced...|\n",
      "|2009-04-07 10:50:19|robrobbierobert|@octolinz16 It it...| it it counts idk...|\n",
      "|2009-04-07 10:50:19|           TLeC|@caregiving I cou...| i  bear to watch...|\n",
      "|2009-04-07 10:50:20|    bayofwolves|@smarrison i woul...| i wouldve been t...|\n",
      "|2009-04-07 10:50:20|     HairByJess|@iamjazzyfizzle I...| i wish i got to ...|\n",
      "|2009-04-07 10:50:22| lovesongwriter|Hollis' death sce...|hollis death scen...|\n",
      "|2009-04-07 10:50:25|       armotley|about to file taxes |about to file taxes |\n",
      "|2009-04-07 10:50:31|     starkissed|@LettyA ahh ive a...| ahh ive always w...|\n",
      "|2009-04-07 10:50:34|      gi_gi_bee|@FakerPattyPattz ...| oh dear were you...|\n",
      "+-------------------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove numbers\n",
    "    cleaned_text = regexp_replace(text, r'\\d+', '')\n",
    "    \n",
    "    # Remove mentions\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'@[A-Za-z0-9_]+', '')\n",
    "    \n",
    "    # Remove email addresses\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9+._-]+\\.[a-zA-Z0-9+._-]+)', '')\n",
    "    \n",
    "    # Remove hyperlinks\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'https?://.*[\\r\\n]*', '')\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'http?://.*[\\r\\n]*', '')\n",
    "    \n",
    "    # Remove hashtags\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'#[A-Za-z0-9_]+', '')\n",
    "    \n",
    "    # Remove brackets\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\" ?\\([^)]+\\)\", \"\")\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'[<.*?>]+', '')\n",
    "    \n",
    "    # Remove specific words\n",
    "    words_to_remove = [\n",
    "        \"ain't\", \"aren't\", \"can't\", \"can't've\", \"'cause\", \"couldn't\", \"could've\",\n",
    "        \"didn't\", \"doesn't\", \"don't\", \"hadn't\", \"hadn't've\", \"hasn't\", \"haven't\",\n",
    "        \"he'd\", \"he'd've\", \"he'll\", \"he'll've\", \"he's\", \"how'd\", \"how'd'y\", \"how'll\",\n",
    "        \"how's\", \"i'd\", \"i'd've\", \"i'll\", \"i'll've\", \"i'm\", \"i've\", \"isn't\", \"it'd\",\n",
    "        \"it'd've\", \"it'll\", \"it'll've\", \"it's\", \"let's\", \"ma'am\", \"mayn't\", \"might've\",\n",
    "        \"mightn't\", \"mightn't've\", \"mustn't\", \"mustn't've\", \"needn't\", \"needn't've\",\n",
    "        \"o'clock\", \"oughtn't\", \"oughtn't've\", \"shan't\", \"sha'n't\", \"shan't've\", \"she'd\",\n",
    "        \"she'd've\", \"she'll\", \"she'll've\", \"she's\", \"should've\", \"shouldn't\", \"shouldn't've\",\n",
    "        \"so've\", \"so's\", \"that'd\", \"that'd've\", \"that's\", \"there'd\", \"there'd've\", \"there's\",\n",
    "        \"they'd\", \"they'd've\", \"they'll\", \"they'll've\", \"they're\", \"they've\", \"to've\", \"wasn't\",\n",
    "        \"we'd\", \"we'd've\", \"we'll\", \"we'll've\", \"we're\", \"we've\", \"weren't\", \"what'll\"\n",
    "    ]\n",
    "    \n",
    "    for word in words_to_remove:\n",
    "        cleaned_text = regexp_replace(cleaned_text, r'\\b{}\\b'.format(word), '')\n",
    "    \n",
    "    # Remove special characters\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'([_|!|%|^|&|*^|\\|~|=|$\\|/|.,!?/:;\\\"\\'“\\”\\’]+)', '')\n",
    "    \n",
    "    # Remove double quotes\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'[\"\"]', '')\n",
    "    \n",
    "    # Remove dots\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'[.|.^]+', '')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    cleaned_text = lower(cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "cleaned_data = sorted_data.withColumn('cleaned_text', clean_text(col('text')))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "cleaned_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Install spaCy model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text_with_spacy(text):\n",
    "    # Lowercasing, removing mentions, hyperlinks, and special characters\n",
    "    cleaned_text = ' '.join(TextBlob(text).words.lower())\n",
    "    \n",
    "    # Tokenize words\n",
    "    words = cleaned_text.split()\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(nlp.Defaults.stop_words)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Apply stemming\n",
    "    words = [nlp(word)[0].lemma_ for word in words]\n",
    "\n",
    "    # Join the cleaned and processed words\n",
    "    cleaned_text = ' '.join(words)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "cleaned_data = cleaned_data.withColumn('cleaned_text', clean_text_with_spacy(col('cleaned_text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the resulting DataFrame\n",
    "cleaned_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+\n",
      "|               date|           user|        cleaned_text|\n",
      "+-------------------+---------------+--------------------+\n",
      "|2009-04-07 10:49:45|_TheSpecialOne_|                    |\n",
      "|2009-04-07 10:49:49|  scotthamilton|is upset that he ...|\n",
      "|2009-04-07 10:49:53|       mattycus| i dived many tim...|\n",
      "|2009-04-07 10:49:57|        ElleCTF|my whole body fee...|\n",
      "|2009-04-07 10:49:57|         Karoli| no  not behaving...|\n",
      "+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the first three columns and create a new DataFrame\n",
    "cleaned_data = cleaned_data.select(\"date\", \"user\", \"cleaned_text\")\n",
    "cleaned_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in cleaned_text: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in a specific column (e.g., \"columnName\")\n",
    "column_name = \"cleaned_text\"\n",
    "null_count_in_column = cleaned_data.filter(col(column_name).isNull()).count()\n",
    "\n",
    "# Show the count of null values in the specific column\n",
    "print(f\"Number of null values in {column_name}: {null_count_in_column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saving Cleaned Data into MongoDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MySQLWriteExample\").getOrCreate()\n",
    "\n",
    "# Specify the MongoDB connection details\n",
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "database_name = \"Big_Tweet\"\n",
    "collection_name = \"Cleaned_Data\"\n",
    "\n",
    "# Write the DataFrame to MongoDB\n",
    "cleaned_data.write.format(\"mongo\").mode(\"overwrite\").option(\"uri\", mongo_uri + database_name + \".\" + collection_name).save()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saving Cleaned Data into Mysql**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MySQLWriteExample\").getOrCreate()\n",
    "\n",
    "# Assuming 'df' is your DataFrame that you want to write to MySQL\n",
    "\n",
    "# JDBC connection properties for MySQL\n",
    "mysql_properties = {\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"url\": \"jdbc:mysql://localhost:3306/Big_Data\",\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root1234\",\n",
    "    \"dbtable\": \"cleaned_data\",\n",
    "}\n",
    "\n",
    "# Write DataFrame to MySQL\n",
    "cleaned_data.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", mysql_properties[\"url\"]) \\\n",
    "    .option(\"driver\", mysql_properties[\"driver\"]) \\\n",
    "    .option(\"dbtable\", mysql_properties[\"dbtable\"]) \\\n",
    "    .option(\"user\", mysql_properties[\"user\"]) \\\n",
    "    .option(\"password\", mysql_properties[\"password\"]) \\\n",
    "    .mode(\"overwrite\")  # Change to \"append\" if needed\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saving Cleaned Data into CSV_File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the CSV file\n",
    "csv_path = \"Cleaned_Data.csv\"\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "#cleaned_data.write.csv(csv_path, header=True, mode=\"overwrite\")\n",
    "cleaned_data.coalesce(1).write.csv(csv_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for the cleaning the text Define the cleaning function\n",
    "#def clean_text(text):\n",
    "#\n",
    "#    #  for the Remove numbers\n",
    "#    cleaned_text = regexp_replace(text, r'\\d+', '')\n",
    "#    \n",
    "#    # for the Remove mentions\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r'@[A-Za-z0-9_]+', '')\n",
    "#    \n",
    "#    #  for the Remove email addresses\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9+._-]+\\.[a-zA-Z0-9+._-]+)', '')\n",
    "#    \n",
    "#    # for the Remove hyperlinks\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r'https?:\\/\\/.*[\\r\\n]*', '')\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r'http?:\\/\\/.*[\\r\\n]*', '')\n",
    "#    \n",
    "#    #  for the Remove hashtags\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r'#[A-Za-z0-9_]+', '')\n",
    "#    \n",
    "#    # Remove brackets\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r\" ?\\([^)]+\\)\", \"\")\n",
    "#    \n",
    "#    #  for the Remove HTML tags\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r'[<.*?>]+', '')\n",
    "#    \n",
    "#    # Remove specific words\n",
    "#    words_to_remove = [\n",
    "#        \"ain't\", \"aren't\", \"can't\", \"can't've\", \"'cause\", \"couldn't\", \"could've\",\n",
    "#        \"didn't\", \"doesn't\", \"don't\", \"hadn't\", \"hadn't've\", \"hasn't\", \"haven't\",\n",
    "#        \"he'd\", \"he'd've\", \"he'll\", \"he'll've\", \"he's\", \"how'd\", \"how'd'y\", \"how'll\",\n",
    "#        \"how's\", \"i'd\", \"i'd've\", \"i'll\", \"i'll've\", \"i'm\", \"i've\", \"isn't\", \"it'd\",\n",
    "#        \"it'd've\", \"it'll\", \"it'll've\", \"it's\", \"let's\", \"ma'am\", \"mayn't\", \"might've\",\n",
    "#        \"mightn't\", \"mightn't've\", \"mustn't\", \"mustn't've\", \"needn't\", \"needn't've\",\n",
    "#        \"o'clock\", \"oughtn't\", \"oughtn't've\", \"shan't\", \"sha'n't\", \"shan't've\", \"she'd\",\n",
    "#        \"she'd've\", \"she'll\", \"she'll've\", \"she's\", \"should've\", \"shouldn't\", \"shouldn't've\",\n",
    "#        \"so've\", \"so's\", \"that'd\", \"that'd've\", \"that's\", \"there'd\", \"there'd've\", \"there's\",\n",
    "#        \"they'd\", \"they'd've\", \"they'll\", \"they'll've\", \"they're\", \"they've\", \"to've\", \"wasn't\",\n",
    "#        \"we'd\", \"we'd've\", \"we'll\", \"we'll've\", \"we're\", \"we've\", \"weren't\", \"what'll\"\n",
    "#    ]\n",
    "#    \n",
    "#    for word in words_to_remove:\n",
    "#        cleaned_text = regexp_replace(cleaned_text, r'\\b{}\\b'.format(word), '')\n",
    "#    #  for the Remove special characters\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r'([_|!|%|^|&|*^|\\|~|=|$\\|/|.,!?/:;\\\"\\'\\“\\”\\’]+)', '')\n",
    "#    \n",
    "#    #  for the Remove double quotes\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r'[\"\"]', '')\n",
    "#    \n",
    "#    #  for the Remove dots\n",
    "#    cleaned_text = regexp_replace(cleaned_text, r'[.|.^]+', '')\n",
    "#    \n",
    "#    return cleaned_text\n",
    "#\n",
    "## now we Apply the cleaning function to the text column\n",
    "#df = sorted_data.withColumn('cleaned_text', clean_text(col('text')))\n",
    "#\n",
    "##  resulting DataFrame\n",
    "#df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
