{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Access the MongoDB URI from the environment variables\n",
    "mongodb_uri = os.getenv(\"uri\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Establish a connection to the MongoDB server\n",
    "\n",
    "# client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "client = MongoClient(mongodb_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c66782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the desired database and collection\n",
    "db = client[\"Big_Tweet\"]\n",
    "collection = db[\"Tweets\"]\n",
    "\n",
    "# Fetch the data from the collection\n",
    "data = list(collection.find())\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a CSV file\n",
    "df = spark.read.csv(\"Timeseries_tweets.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "#spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Reading a CSV file directly into a Spark DataFrame\n",
    "#spark_df = spark.read.csv(\"Timeseries_tweets.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b948e2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  droping unnecessary columns\n",
    "df=df.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda6ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a look of data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea01dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printing data schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b691761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to change the date in a proper format importing necessary module\n",
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e016ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the date in year-month-day format\n",
    "df = df.withColumn('date', date_format(df['date'], 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5b1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the tweets cleaning importing all necessary module that help us to work with text data\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.sql.functions import col, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for the cleaning the text Define the cleaning function\n",
    "def clean_text(text):\n",
    "    #  for the Remove numbers\n",
    "    cleaned_text = regexp_replace(text, r'\\d+', '')\n",
    "    \n",
    "    # for the Remove mentions\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'@[A-Za-z0-9_]+', '')\n",
    "    \n",
    "    #  for the Remove email addresses\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9+._-]+\\.[a-zA-Z0-9+._-]+)', '')\n",
    "    \n",
    "    # for the Remove hyperlinks\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'https?:\\/\\/.*[\\r\\n]*', '')\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'http?:\\/\\/.*[\\r\\n]*', '')\n",
    "    \n",
    "    #  for the Remove hashtags\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'#[A-Za-z0-9_]+', '')\n",
    "    \n",
    "    # Remove brackets\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\" ?\\([^)]+\\)\", \"\")\n",
    "    \n",
    "    #  for the Remove HTML tags\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'[<.*?>]+', '')\n",
    "    \n",
    "    #  TO Remove apostrophe words\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"ain\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"aren\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"can\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"can't've\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"'cause\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"couldn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"could\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"couldn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"couldn\\'t've\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"didn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"doesn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"don\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"hadn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"hadn\\'t\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"hasn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"haven\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"he\\'d\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"he\\'d\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"he\\'ll\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"he\\'ll've\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"he\\'s\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"how\\'d\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"how\\'d\\'y\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"how\\'ll\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"how\\'s\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"i\\'d\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"i\\'d\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"i\\'ll\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"i\\'ll\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"i\\'m\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"i\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"isn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"it\\'d\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"it\\'d\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"it\\'ll\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"it\\'ll\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"it\\'s\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"let\\'s\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"ma\\'am\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"mayn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"might\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"mightn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"mightn\\'t\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"mustn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"mustn\\'t\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"needn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"needn\\'t\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"o\\'clock\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"oughtn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"oughtn\\'t\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"shan\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"sha\\'n\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"shan\\'t\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"she\\'d\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"she\\'d\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"she\\'ll\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"she\\'ll\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"she\\'s\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"should\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"shouldn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"shouldn\\'t\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"so\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"so\\'s\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"that\\'d\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"that\\'d\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"that\\'s\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"there\\'d\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"there\\'d\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"there\\'s\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"they\\'d\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"they\\'d\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"they\\'ll\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"they\\'ll\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"they\\'re\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"they\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"to\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"wasn\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"we\\'d\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"we\\'d\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"we\\'ll\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"we\\'ll\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"we\\'re\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"we\\'ve\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"weren\\'t\", \"\")\n",
    "    cleaned_text = regexp_replace(cleaned_text, r\"what\\'ll\", \"\")\n",
    "    #  for the Remove special characters\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'([_|!|%|^|&|*^|\\|~|=|$\\|/|.,!?/:;\\\"\\'\\“\\”\\’]+)', '')\n",
    "    \n",
    "    #  for the Remove double quotes\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'[\"\"]', '')\n",
    "    \n",
    "    #  for the Remove dots\n",
    "    cleaned_text = regexp_replace(cleaned_text, r'[.|.^]+', '')\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# now we Apply the cleaning function to the text column\n",
    "df = df.withColumn('cleaned_text', clean_text(col('text')))\n",
    "\n",
    "#  resulting DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e718fc59",
   "metadata": {},
   "source": [
    "### Storing the cleaned data into Mongo Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c67c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving clean data to a object\n",
    "clean_text=df.select('Date','cleaned_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff886b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data to pandas data frame for the forcasting purpuse\n",
    "clean_df = clean_text.toPandas()\n",
    "clean_df.to_csv(\"clean_timeseries_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data into disctionary  \n",
    "clean_dt = clean_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Creating a collection in the database\n",
    "db.Cleaned_Tweets.insert_many(clean_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12800ed0",
   "metadata": {},
   "source": [
    "### Reading cleaned data from mongo database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the clean tweet for analysis and forecasting the sentiment lable\n",
    "#tweets=pd.read_csv(\"clean_timeseries_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edda187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the desired database and collection\n",
    "db = client[\"Big_Tweet\"]\n",
    "collection = db[\"Cleaned_Tweets\"]\n",
    "\n",
    "# Fetch the data from the collection\n",
    "data = list(collection.find())\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "tweets = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbf7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null value\n",
    "tweets.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167abc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping the null value\n",
    "tweets.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba287a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the text analyzer module called textblob\n",
    "from textblob import *\n",
    "\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return  TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Create two new columns 'Subjectivity' & 'Polarity'\n",
    "tweets['Subjectivity'] = tweets['cleaned_text'].apply(getSubjectivity)\n",
    "tweets['Polarity'] = tweets['cleaned_text'].apply(getPolarity)\n",
    "\n",
    "def getAnalysis(score):\n",
    "      if score < 0:\n",
    "        return 'Negative'\n",
    "      elif score == 0:\n",
    "        return 'Neutral'\n",
    "      else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the sentiment label based on Polarity of text\n",
    "tweets['Sentiment'] = tweets['Polarity'].apply(getAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968368b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the label to the corresponding sentiment label.\n",
    "tweets['Sentiment_label']=tweets['Sentiment'].map({'Positive':1,'Neutral':0,'Negative':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442928ca",
   "metadata": {},
   "source": [
    "### `Ploting the sentiment of people over the time on tweeter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af425f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Group the data by date and sentiment category and count the occurrences\n",
    "sentiment_counts = tweets.groupby(['Date', 'Sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create a list of colors for each sentiment category\n",
    "colors = {'Positive': 'green', 'Negative': 'red', 'Neutral': 'blue'}\n",
    "\n",
    "# Create a line chart for each sentiment category\n",
    "fig = go.Figure()\n",
    "for sentiment in sentiment_counts.columns:\n",
    "    fig.add_trace(go.Scatter(x=sentiment_counts.index, y=sentiment_counts[sentiment],\n",
    "                             mode='lines',\n",
    "                             name=sentiment,\n",
    "                             line=dict(color=colors[sentiment]),\n",
    "                             marker=dict(symbol='circle', size=8)))\n",
    "    \n",
    "# Update layout\n",
    "fig.update_layout(title='Sentiment Over Time',\n",
    "                  xaxis_title='Date',\n",
    "                  yaxis_title='Count',\n",
    "                  legend_title='Sentiment')\n",
    "\n",
    "# Set figure size\n",
    "fig.update_layout(width=1200, height=600)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13d92c",
   "metadata": {},
   "source": [
    "### `Ploting the sentiment of people over the week on tweeter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1dec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the index to a datetime type\n",
    "sentiment_counts.index = pd.to_datetime(sentiment_counts.index)\n",
    "\n",
    "# Create a line chart for each sentiment category\n",
    "fig = go.Figure()\n",
    "for sentiment in sentiment_counts.columns:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sentiment_counts.index,\n",
    "        y=sentiment_counts[sentiment],\n",
    "        mode='lines',\n",
    "        name=sentiment,\n",
    "        line=dict(color=colors[sentiment], width=2),\n",
    "        hovertemplate=': %{x}<br>' +\n",
    "                      'Sentiment: ' + sentiment + '<br>' +\n",
    "                      'Count: %{y}<br>' +\n",
    "                      'Percentage: %{text}%<extra></extra>',\n",
    "        text=((sentiment_counts[sentiment] / sentiment_counts.sum(axis=1)) * 100).round(2)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Sentiment Over Weeks',\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tickangle=45,\n",
    "        showticklabels=False,\n",
    "        dtick='7D',  # Set tick frequency to one week (7 days)\n",
    "        tickformat='%Y-%m-%d'  # Format the date as desired\n",
    "    ),\n",
    "    xaxis_title='',\n",
    "    yaxis_title='Count',\n",
    "    legend_title='Sentiment'\n",
    ")\n",
    "\n",
    "# Set figure size\n",
    "fig.update_layout(width=1200, height=600)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403452de",
   "metadata": {},
   "source": [
    "### `Ploting the sentiment of people over the Month on tweeter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index to a datetime type\n",
    "sentiment_counts.index = pd.to_datetime(sentiment_counts.index)\n",
    "\n",
    "# Create a line chart for each sentiment category\n",
    "fig = go.Figure()\n",
    "for sentiment in sentiment_counts.columns:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sentiment_counts.index,\n",
    "        y=sentiment_counts[sentiment],\n",
    "        mode='lines',\n",
    "        name=sentiment,\n",
    "        line=dict(color=colors[sentiment], width=2),\n",
    "        hovertemplate=': %{x}<br>' +\n",
    "                      'Sentiment: ' + sentiment + '<br>' +\n",
    "                      'Count: %{y}<br>' +\n",
    "                      'Percentage: %{text}%<extra></extra>',\n",
    "        text=((sentiment_counts[sentiment] / sentiment_counts.sum(axis=1)) * 100).round(2)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Sentiment Over Months',\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tickangle=45,\n",
    "        showticklabels=True,\n",
    "        dtick='M1',  # Set tick frequency to one month\n",
    "        tickformat='%Y-%m-%d'  # Format the date as desired\n",
    "    ),\n",
    "    xaxis_title='Month',\n",
    "    yaxis_title='Count',\n",
    "    legend_title='Sentiment'\n",
    ")\n",
    "\n",
    "# Set figure size\n",
    "fig.update_layout(width=1200, height=600)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### `Ploting the sentiment of people over the Quater on tweeter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Date' is the datetime column in your 'tweets' DataFrame\n",
    "tweets['Date'] = pd.to_datetime(tweets['Date'])\n",
    "\n",
    "# Set 'Date' as the index\n",
    "tweets.set_index('Date', inplace=True)\n",
    "\n",
    "# Group the data by quarter and sentiment category and count the occurrences\n",
    "sentiment_counts_quarterly = tweets.groupby([pd.Grouper(freq='Q'), 'Sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create a new datetime index\n",
    "quarterly_dates = pd.date_range(start=sentiment_counts_quarterly.index.min(), end=sentiment_counts_quarterly.index.max(), freq='Q')\n",
    "\n",
    "# Assign the new datetime index to sentiment_counts_quarterly\n",
    "sentiment_counts_quarterly.index = quarterly_dates\n",
    "\n",
    "# Now, create a line chart for each sentiment category\n",
    "fig_quarterly = go.Figure()\n",
    "for sentiment in sentiment_counts_quarterly.columns:\n",
    "    fig_quarterly.add_trace(go.Scatter(\n",
    "        x=sentiment_counts_quarterly.index,\n",
    "        y=sentiment_counts_quarterly[sentiment],\n",
    "        mode='lines',\n",
    "        name=sentiment,\n",
    "        line=dict(color=colors[sentiment], width=2),\n",
    "        hovertemplate='Quarter: %{x}<br>' +\n",
    "                      'Sentiment: ' + sentiment + '<br>' +\n",
    "                      'Count: %{y}<br>' +\n",
    "                      'Percentage: %{text}%<extra></extra>',\n",
    "        text=((sentiment_counts_quarterly[sentiment] / sentiment_counts_quarterly.sum(axis=1)) * 100).round(2)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig_quarterly.update_layout(\n",
    "    title='Sentiment Over Quarters',\n",
    "    xaxis=dict(\n",
    "        tickmode='array',\n",
    "        tickvals=sentiment_counts_quarterly.index,\n",
    "        ticktext=sentiment_counts_quarterly.index.strftime('%b %Y'),\n",
    "        showticklabels=True\n",
    "    ),\n",
    "    xaxis_title='Quarter',\n",
    "    yaxis_title='Count',\n",
    "    legend_title='Sentiment'\n",
    ")\n",
    "\n",
    "# Set figure size\n",
    "fig_quarterly.update_layout(width=1200, height=600)\n",
    "\n",
    "# Show the plot\n",
    "fig_quarterly.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f38eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6581d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
